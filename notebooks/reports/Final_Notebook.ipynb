{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate 5, Get 5 - Recommendation System Project \n",
    "\n",
    "by Leana Critchell, Jacob Prebys and Dann Morr\n",
    "\n",
    "\n",
    "<img src=\"../../src/figures/movielens_logo.png\" alt=\"drawing\" width=\"250\"/>\n",
    "\n",
    "## Table of Contents\n",
    "- [Overview](#Overview)\n",
    "- [Data Cleaning and Exploratory Data Analysis](#Data-Cleaning-and-Exploratory-Data-Analysis)\n",
    "- [Models](#Models)\n",
    "  - [Collaborative Filtering Model](#Collaborative-Filtering-Model)\n",
    "  - [Content-Based Model](#Content-Based-Model)\n",
    "- [Final Results](#Final-Results)\n",
    "- [Future Work](#Future-Work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We aim to create a recommendation system based on the MovieLens dataset from the GroupLens research lab at the University of Minnesota. Furthermore, we would like to deploy a web app that will alloy a user to enter some ratings for movies that they have seen, and then, based on the model we have implemented, it will reccomend movies that align with their interests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling and processing imports\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import accuracy\n",
    "\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import SVDpp\n",
    "from surprise.prediction_algorithms import SlopeOne\n",
    "from surprise.prediction_algorithms import NMF\n",
    "from surprise.prediction_algorithms import NormalPredictor\n",
    "from surprise.prediction_algorithms import KNNBaseline\n",
    "from surprise.prediction_algorithms import KNNBasic\n",
    "from surprise.prediction_algorithms import KNNWithMeans\n",
    "from surprise.prediction_algorithms import KNNWithZScore\n",
    "from surprise.prediction_algorithms import BaselineOnly\n",
    "from surprise.prediction_algorithms import CoClustering\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parameters\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 25\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['axes.edgecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white' # or EAEAF2\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "parent_dir = '../../'\n",
    "\n",
    "from src import recommender as rec\n",
    "from src import content_rec as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "The data used for this project is from GroupLens and is called the MovieLens Dataset.  You can find all the details of this dataset and download the appropriate data files yourself [here](https://grouplens.org/datasets/movielens/latest/).  \n",
    "\n",
    "Alternatively, you can click [this link](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip) to download the zip file of the data files used in this project (1MB).  This zip file contains 4 csv files:  `movies`, `ratings`, `tags` and `links`.  See the README.md in the [data](../../data) folder for more info on how this data is formatted.  On the website provided above,  you also have access to the 'large' dataset which is 256MB and was not used in this project.  Download from their website at your own will.  \n",
    "\n",
    "The four csv datasets were downloaded to this repo which you can find [here](../../data) - they are labelled `movies.csv`, `links.csv`, `ratings.csv` and `tags.csv`.  If you're following along in this notebook, the cells below will run as we import these csv's using pandas.  Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in 4 datasets:\n",
    "ratings = pd.read_csv(parent_dir + 'data/ratings.csv')\n",
    "\n",
    "movies = pd.read_csv(parent_dir + 'data/movies.csv')\n",
    "\n",
    "tags = pd.read_csv(parent_dir + 'data/tags.csv')\n",
    "\n",
    "links = pd.read_csv(parent_dir + 'data/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "It should be noted that each group member performed their own EDA in their own way.  Please refer to each member's individual exploratory notebooks which you can find [here](../exploratory), for more details on individual findings and explorations.  What will be detailed here is a summary of all of our EDA efforts combined.  \n",
    "\n",
    "We'll start by exploring each dataset and then aggregate as necessary as we go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Dataset\n",
    "\n",
    "Let's first start by looking into the `ratings` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many unique movies we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings.movieId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many unique ratings we have to make sure we don't have any weird values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of ratings: {len(ratings.rating.unique())}\")\n",
    "print(f\"Possible rating values:  {ratings.rating.unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these numbers are reasonable and expected.  We can see that we have a 10 point scale from 0.5 - 5 so half ratings are included and 0 is not included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check data types and null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not appear to have any missing values and all data types seem reasonable.  While we won't use the timestamp column for modelling (this will be dropped), we may want to investigate timeseries information later so we will transform this column to a datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the documentation, `timestamp` is seconds since 1970.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate some more details of the dataframe.  How many users do we have in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of users: {len(ratings.userId.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the average rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average rating:  {ratings.rating.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have an average rating that is just above the median rating (2.5).  Let's have a look at the distribution of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (10, 8))\n",
    "plt.hist(ratings.rating, bins = 10, color = '#789698')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Num. of Ratings')\n",
    "plt.savefig(parent_dir + 'reports/figures/dist_ratings.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see again here that the average rating is around 3.5 and the data is left-skewed.  This shows us that there aren't many low ratings between 0.5 and 2.  Perhaps this says something about the motivation for people to rate movies - perhaps people don't bother if the movie is bad...\n",
    "\n",
    "Let's have a look at the average rating per movies and view this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated = pd.DataFrame(ratings.groupby(['movieId'])['rating'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated.sort_values('rating', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also find the number of ratings for each movie and add it to our new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated['num_rating'] = pd.DataFrame(ratings.groupby(['movieId'])['rating'].count())\n",
    "rated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.hist(rated.rating, bins = 10, color = '#789698')\n",
    "plt.xticks([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5])\n",
    "plt.title('Distribution of Mean Ratings')\n",
    "plt.xlabel('Rating Scale')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.savefig(parent_dir + 'reports/figures/dist_mean_ratings.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar shape here again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Longtail Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that comes up a lot in recommendation system problems is the long tail problem.  This is where we have a fast majority of users and/or items that only have 1 rating associated to them and a small amount of items/users that have a lot of ratings associated with them.  Let's first look into the number of ratings per movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group ratings by movie and count the number of ratings per movie\n",
    "num_ratings = ratings.groupby('movieId').count().drop('userId', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these ratings\n",
    "sorted_num_ratings = num_ratings.sort_values(by = 'rating', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_num_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (10, 8))\n",
    "sns.distplot(sorted_num_ratings.index, bins = 500, color = '#789698')\n",
    "plt.title('Distribution of Number of Ratings per Movie')\n",
    "plt.xlabel('Num. of Ratings per Movie')\n",
    "plt.savefig(parent_dir + 'reports/figures/ratings_by_movie.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we do have a long tail problem here where the majority of movies have less than 25 ratings and very few have more than that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look into the number of ratings per user to investigate this long tail problem further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.DataFrame(ratings.groupby(['userId'])['rating'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.sort_values('rating', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"top 12\" users have each rated over 1000 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.sort_values('rating', ascending=True)[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the flip side around 75 users have rated 25 movies or fewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.hist(users.rating, bins = 200, color = '#789698')\n",
    "plt.title('Number of Ratings by User')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Count of Users')\n",
    "plt.savefig(parent_dir + 'reports/figures/ratings_by_user.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see the long tail problem playing out here.  This will have to be addressed with regularisation in our modelling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look into the movies dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Dataset\n",
    "\n",
    "Let's begin by looking at a preview of our data as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's inspect the datatypes and null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from here we assume there are 9742 unique movies. But let's check the unique titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique movie titles:  {len(movies.title.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't agree with the 9742 we saw earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique movie IDs:  {len(movies['movieId'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 9742 unique movieId's but only 9737 unique titles.  This means some movies have 2 different movieIds.  Let's see if we can isolate these movies (there are only 5).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_movies = {}\n",
    "for title in movies['title']:\n",
    "    count_movies[title] = count_movies.get(title, 0) + 1\n",
    "len(count_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's now see which movies have a count greater than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_movies = []\n",
    "for title in count_movies:\n",
    "    if count_movies[title] > 1:\n",
    "        print(title, count_movies[title])\n",
    "        double_movies.append(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found the duplicates in disguise.  Let's find these in our dataframe to find their movieIds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies['title'].isin(double_movies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in double_movies:\n",
    "    print(movies[movies['title'] == title])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to drop the rows where the genre is only a subset of the duplicate's list of genres.  E.g., I'll drop row 5601 because it only has 'Romance' whereas Romance is included in row 650 of the 'Emma' movie.  \n",
    "\n",
    "Since there are only 5 rows to drop, I'll manually make a list of their index's to drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [5601, 9468, 4169, 5854, 6932]\n",
    "movies.drop(rows_to_drop, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it worked as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_movies_again = {}\n",
    "for title in movies['title']:\n",
    "#     print(movie)\n",
    "    count_movies_again[title] = count_movies_again.get(title, 0) + 1\n",
    "len(count_movies_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_movies_again = []\n",
    "for title in count_movies_again:\n",
    "    if count_movies_again[title] > 1:\n",
    "        print(title, count_movies_again[title])\n",
    "        double_movies_again.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(double_movies_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome.  We now don't have any doubled up movies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate how many unique genre combinations we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique genre combinations:  {len(movies['genres'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of genres - let's get a dictionary containing the count for each genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_genres = {}\n",
    "for genre in movies['genres']:\n",
    "    count_genres[genre] = count_genres.get(genre, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 951 unique genre combinations.  Let's see how many of these only have 1 movie classified as this combination of genres.  Perhaps these are 'less common' or more 'out-there' movies.  Or perhaps their genre could be reduced to be more generalisable.  \n",
    "\n",
    "This is important because we loose information about people who like movies of the same genre, but if someone is classified as 'not alike' just because a genre combination of their favourite movie was 'Adventure|Children|Romance' and another person's was 'Adventure|Children|Romance|IMAX', this could loose valuable information about those people.  \n",
    "\n",
    "Perhaps we'll need to make sure 'genre' is handled appropritely and that our model features include the different types of genres included in combination genres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one = []\n",
    "for genre in count_genres:\n",
    "    if count_genres[genre] == 1:\n",
    "#         print(genre, count_genres[genre])\n",
    "        only_one.append(genre)\n",
    "print(f\"Number of genres with only 1 movie of this genre combination:  {len(only_one)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the most common genres and find the top ten genre combinations (that is, the genre with the most amount of movies listed as this genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_genre = []\n",
    "for genre in count_genres:\n",
    "    if count_genres[genre] > 100:\n",
    "        print(genre, count_genres[genre])\n",
    "        popular_genre.append(genre)\n",
    "print(f\"\\nNumber of genres with more than 100 movies listed as this genre combinations:  {len(popular_genre)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pop_genres = sorted(popular_genre, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pop_genres[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list, we can see that there's pretty much 3 genres that the top movies boil down to:\n",
    "- Drama\n",
    "- Crime/Thriller\n",
    "- Comedy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to make sure we're filtering by unique combinations and maybe we can extract the single-use genre combinations and get rid of their unique extra genre?  These are things we will need to consider for our content-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise these top 10 genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = pd.DataFrame(movies.groupby('genres')['title'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_genre = genre.sort_values('title', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10, 8))\n",
    "plt.barh(most_rated_genre.index, most_rated_genre.title, color = '#789698')\n",
    "plt.title('10 Most Rated Genres')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Genre')\n",
    "plt.savefig(parent_dir + 'reports/figures/top_10_genres.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here that `Drama` is the most highly rated genre, second is `Comedy` and third `Comedy|Drama`.  This along suggests that these could be aggregated some how and should be considered in future investigations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I'll read out the df I have that dropped those duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dropped = movies.to_csv(parent_dir + 'data/mod_movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series with `movies` and `ratings` dataframe:\n",
    "\n",
    "As mentioned earlier, we kept the timestamp coloumn so we could investigate information about the timing of this data.  Let's explore that now by combining the ratings and movies dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['release_year'] = movies.title.apply(lambda x: x.strip()[-5:-1])\n",
    "movies['release_year'] = pd.to_numeric(movies['release_year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_joined = ratings.join(movies.set_index('movieId'), on='movieId').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = movies_ratings_joined.groupby('release_year')['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouproll = grouped.rolling(10).mean()\n",
    "grouproll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.bar(grouproll[1940:].index, grouproll[1940:], linewidth=2, color='#789698')\n",
    "plt.title('Ratings by Release Year')\n",
    "ax.set_ylim([3.2,4])\n",
    "ax.set_ylabel('Rating')\n",
    "ax.set_xlabel('Release Year')\n",
    "ax.set_xlim([1941, 2018])\n",
    "plt.savefig(parent_dir + 'reports/figures/ratings_by_release_date.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can see that movies that were released before 1990 tend to have a higher average rating.  From roughly 1990, the average movie rating appears to trend downwards towards the average rating of the dataset (3.5).  Since the rating of these movies have taken place since 1993, this could suggest that people who watched and rated older movies, watched them because they were already a recommended to them as being good movies and so these movies are watched by good referral.  Whereas from 1993, movies could have been watched and rated by people's own motivations rather than personal recommendations.  So perhaps this suggests what we see in the data here.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't end up using the timestamp, let's drop it from our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.drop('timestamp', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this df out so it's accessible later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_model = ratings.to_csv(parent_dir + 'data/mod_ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links Dataset\n",
    "\n",
    "Let's get accquainted with the links dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are going to be duplicates again given that there are the same number of `movieId`'s that the movies df had... so let's see if there are duplicate `imdbIds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links.movieId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links.imdbId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok no, doesn't look like there are duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the na's for tmdbId:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links.tmdbId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[links['tmdbId'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's fine that we're missing tmdbIds since they shouldn't add much value to our modelling.  \n",
    "\n",
    "Overall, the `links` dataset will be useful for webscraping if we want to get images from IMDB for the movies to add to deployment methods but it won't add any value to our models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags Dataset\n",
    "\n",
    "Let's get accquainted with the tags dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of users who provided tags:  {len(tags.userId.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique movies with tags:  {len(tags.movieId.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique tags:  {len(tags.tag.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while there are 3683 entries, there are only 1589 unique tags so we do have some common tags - might be worth finding the top 10-20 most common tags perhaps?\n",
    "\n",
    "Only 1572 movies have been tagged so if we join these dfs, most will have na values (which is fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 58 users actually added tags.  This is quiet a small subset of our overall users.\n",
    "\n",
    "We probably don't need the timestamp column for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.drop('timestamp', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the most common tags and find the top 20 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tags = {}\n",
    "for tag in tags['tag']:\n",
    "    count_tags[tag] = count_tags.get(tag, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_tag = []\n",
    "for tag in count_tags:\n",
    "    if count_tags[tag] > 0:\n",
    "        popular_tag.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pop_tags = sorted(popular_tag, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pop_tags[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there's a lot of double ups here with tags that do and don't use capitilisation such asa 'Ryan Reynolds' vs. 'ryan reynolds'.  As well as similar categories such as 'myth', 'mythology', even 'mystery'.  \n",
    "\n",
    "Perhaps we could perform some NLP pre-processing on this data to make more consistent tags.  This might not be completely neccessary since it's such a small set of the data that is tagged (only 3000 amongst 100k movie ratings) but something we could experiment with.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't do any of the NLP processing now, but we know it exists and I will export the csv without the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_model = tags.to_csv(parent_dir + 'data/mod_tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ratings` df has over 100k rows, and then `movies` df has just under 10k rows.  So let's first try joining the `ratings` and `movies` dfs together using `movieId` as the key.  We will left join on `ratings`.\n",
    "\n",
    "First, we'll check the shape of both dfs to be able to compare the joined result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = ratings.set_index('movieId').join(movies.set_index('movieId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking like our desired result.  Let's check the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have not lost rows - this is what we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that there are 20 movies that we do not know the title or genre for.  Let's see what these movies are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings[movie_ratings['title'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, out of the 20 that do not have titles or genres, there are actually only 5 movies that are missing their title.  We could pair this with the links df and look up the title and genre on IMDB and manually add this in since it's only 5 records.  This might be worth it since movie 6003 has 15 ratings that we don't know the name of (and hence can't recommend the name in our app). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now join the links df with this df, again with the movie id as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_links = movie_ratings.join(links.set_index('movieId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_links.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No info loss! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at those movies that didn't have titles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summary \n",
    "\n",
    "Overall, our datasets are pretty clean.  There are definitely areas that will need to be addressed in our modelling such as adding regularisation to account for the long tail problem as well as doing some NLP processing to deal with the genres data for doing content-based models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering Model\n",
    "\n",
    "The key idea behind collaborative filtering is that similar users share similar interests and that users tend to like items that are similar to one another. We plan to use this for our recommendation system. A user will rate 5 movies, that new data will be used to generate recommendations based on the ratings from users in our datset. \n",
    "\n",
    "\n",
    "\n",
    " 1. **Determine the model to use**\n",
    "   - We performed a train test split on our data, then compared several models in their default state to see which would return the best RMSE score. In this test, the best performing model was SVDpp  - The SVD++ algorithm, an extension of SVD taking into account both explicit and implicit ratings.\n",
    "   \n",
    "   \n",
    " 2. **Iterating and tuning the model**\n",
    "  - After the model was chosen we ran several iterations, tuning the hyperparameters each time to see if we could imporve the score.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the joined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../../data/joined_dfs_lc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Reader and the rating scale\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# Load the dataset \n",
    "data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best algorithm to use\n",
    "\n",
    "Research lead me to an article by Susan Li (see references), who provided a method to test a variety of algorithms at once to determine the best option.\n",
    "\n",
    "This will instantiate 11 different models, cross validate the results, then save them all in a dataframe called `benchmark` to compare the RMSE.\n",
    "\n",
    "I'm going to iterate over all the algorithms to see which one returns the best RMSE value.\n",
    "This will take a while to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you to Susan Li for this helpful code\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), \n",
    "                  KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), \n",
    "                  BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT: SVDpp has the lowest RMSE. This will be the model we use.\n",
    "\n",
    "    The SVD++ algorithm is an extension of SVD that takes into account implicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FSM\n",
    "Running SVDpp at default settings and cross-validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick the algorithm and run the first model on its own\n",
    "algo = SVDpp()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration and hypertuning\n",
    "adjusted n_factors to 50, and regularization to 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune\n",
    "algo3 = SVDpp(n_factors=50, reg_all=0.05, verbose=False)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo3.fit(trainset)\n",
    "predictions = algo3.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight improved to RMSE score. Final model used was the 5th iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model\n",
    "adding an adjusted learning rate of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune\n",
    "algo5 = SVDpp(n_factors=50, reg_all=0.05, lr_all=0.01, verbose=False)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo5.fit(trainset)\n",
    "predictions = algo5.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model returned a RMSE score of approx 0.855. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Model\n",
    "\n",
    "The next type of recommendation system we wanted to explore was a content-based version. Our previous model would look at other users that have similar interests, and it would recommend other titles that they have liked. This system goes the other direction and it takes movies that you like, and, having learned some information about the film, recommends titles that are similar to it.\n",
    "\n",
    "To do this, we gathered descriptions and genre tags for each film, and then utilized some of Python's natural language processing tools to turn this text information into numerical information. We used the following process:\n",
    "\n",
    " 1. **TF-IDF Vectorization**\n",
    "   - Short for Term Frequency - Inverse Document Frequency, this is a method for assigning values to each word based on the amount of times it appear in documents. This specific value takes in to account the number of times a word appears in a single description and also how commonly it appears in all descriptions. In a single description, a word is given a high tf-idf score if it appears many times in one description, but it is relatively uncommon across all descriptions. This is partially meant to filter out words that are common to movies in general.\n",
    "   \n",
    "   \n",
    " 2. **Cosine Similarity**\n",
    "  - Once each film is represented by a many-dimensional vector, a common method for determining how 'similar' two films are is by caluculating how close to 1 the cosine of the angle between them is.\n",
    "  \n",
    "  \n",
    " 3. **Sorting**\n",
    "  - Now that we have a measure of similarity between every pair of movies, we can take in a single movie, sort the rest of the movies by how similar they are to our chosen film, and then return the top 10 most similar films.\n",
    "  \n",
    "  \n",
    "We have put together a Python class to demonstrate our content-based recommender, the source code for it can be found in the src folder under the name [content_rec.py](../../src/content_rec.py). Below we initialize the ContentRecommender object and provide some examples of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = cr.ContentRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.recommend('Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.recommend('Thor (2011)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.recommend('Journey 2: The Mysterious Island (2012)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to see some random recommendations, we have included the following method to generate suggestions based off random titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_film = content.random_title()\n",
    "content.recommend(random_film)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system seems to be working out well! We could further improve the recommendations we are seeing by including more descriptive informations. Some additional information might be useful could be cast and crew names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Final Results\n",
    "\n",
    "We had good success with both collaborative and content-based recommendation systems, as well as our Flask deployment. Our final collaborative model ended up with a RMSE of approx 0.855, which is not bad on a 5-point rating scale. Our content based model is showing very good variety in picking movies that are similar in genre and description.\n",
    "\n",
    "## Future Work\n",
    "\n",
    "A good place to direct our efforts in the future would be speeding up our model training process so our app deployment can work faster. We should also consider taking parts of our content and collaboration systems to make a hybrid recommender system that makes SUPER GOOD recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_env",
   "language": "python",
   "name": "rec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
