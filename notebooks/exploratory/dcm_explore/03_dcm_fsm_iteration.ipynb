{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to get down to some modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import accuracy\n",
    "\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import SVDpp\n",
    "from surprise.prediction_algorithms import SlopeOne\n",
    "from surprise.prediction_algorithms import NMF\n",
    "from surprise.prediction_algorithms import NormalPredictor\n",
    "from surprise.prediction_algorithms import KNNBaseline\n",
    "from surprise.prediction_algorithms import KNNBasic\n",
    "from surprise.prediction_algorithms import KNNWithMeans\n",
    "from surprise.prediction_algorithms import KNNWithZScore\n",
    "from surprise.prediction_algorithms import BaselineOnly\n",
    "from surprise.prediction_algorithms import CoClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the joined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../../../data/joined_dfs_lc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start FSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Reader and the rating scale\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# load the dataset \n",
    "data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# sample random trainset and testset\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best algorithm to use\n",
    "\n",
    "Research lead me to an article by Susan Li, who provided a method to test a variety of algorithms at once to determine the best option.\n",
    "\n",
    "I'm going to iterate over all the algorithms to see which one returns the best RMSE value.\n",
    "This one will take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you to Susan Li for this helpful code\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), \n",
    "                  KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), \n",
    "                  BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT: SVDpp has the lowest RMSE... and the longest test time (hooray).\n",
    "I'm going to start with that as my model.\n",
    "\n",
    "    The SVDpp algorithm is an extension of SVD that takes into account implicit ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick the algorithm and run the first model on its own\n",
    "algo = SVDpp(random_state=15)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It definitely takes a while to run, especially with the cross-validation. I will probably leave that out of every iteration for the sake of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I go on, just a quick test to see that it is working as we want it to.\n",
    "Let's get a rating prediction for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.predict(2,60756)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Time to iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I've duplicated this notebook and named the copy '04_dcm_iteration'.\n",
    "I don't know how much time it may buy me, but I am going to run iterations in both notebookswith staggered start times. Even numbers in notebook 04, odd numbers in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3\n",
    "increased n_factors to 50 and regularization to ~~0.005~~ 0.05\n",
    "\n",
    "_NOTE: a typo and a poor choice to copy and paste lead to decreasing the regularization when I intended to increase it. The model continued to improve nonethless_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune\n",
    "algo3 = SVDpp(n_factors=50, reg_all=0.05, verbose=True, random_state=15)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo3.fit(trainset)\n",
    "predictions = algo3.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the RMSE and MAE scores are getting smaller bit by bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 5\n",
    "adding an adjusted learning rate of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune\n",
    "algo5 = SVDpp(n_factors=50, reg_all=0.05, lr_all=0.01, verbose=False, random_state=15)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo5.fit(trainset)\n",
    "predictions = algo5.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of .8551... we were hoping to get to .86 so this is a little bonus.\n",
    "\n",
    "We need to test the prototype app, so I am going to pickle this and we will try it with the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do want to make sure and cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo5, data, measures=['RMSE'], cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i'm going to pickle this so we can test the prototype app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../../model_files/SVDpp.bin\", 'wb') as f_out:\n",
    "    pickle.dump(algo5, f_out) \n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at this model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you again to Susan Li for this helpful code\n",
    "\n",
    "def get_Iu(uid):\n",
    "    \"\"\" return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "    \n",
    "def get_Ui(iid):\n",
    "    \"\"\" return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "df['Iu'] = df.uid.apply(get_Iu)\n",
    "df['Ui'] = df.iid.apply(get_Ui)\n",
    "df['err'] = abs(df.est - df.rui)\n",
    "best_predictions = df.sort_values(by='err')[:100]\n",
    "worst_predictions = df.sort_values(by='err')[-100:]\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "worst_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_env",
   "language": "python",
   "name": "rec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
